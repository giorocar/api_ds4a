{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing\n",
    "This step is for creating a text preprocessed column of descripcion del proceso(Objeto a contratar). This may take a while. Is necessary for creating the tokenizer. the code is partitioned in some steps and create some other csv and binary files due to high ram memory consumption because of the large size of the DataSet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from unidecode import unidecode\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## secop.csv is the resulting dataset from union of secopI and secopII datasets.\n",
    "df = pd.read_csv('datos/secop.csv', low_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = unidecode(' '.join(stopwords.words(\"spanish\")))\n",
    "stopwords = stopwords.split()\n",
    "stopwords = stopwords + ['y'] +['o'] + ['s'] + ['b'] + ['c'] + ['i']\n",
    "otras=['s', 'asi', 'demas', 'p', 'x' ,'ndeg', 'c', 'n', 'i', 'd', 'm','ii' , 'pic', 'th' ,'b', 'iv', 'np', 'ebi', 'u', 'ml', 'l']\n",
    "\n",
    "stopwords = stopwords + otras\n",
    "\n",
    "ciudad = df['ciudad'].str.lower().unique().tolist()\n",
    "ciudad = unidecode(' '.join(ciudad))\n",
    "ciudad = ciudad.replace('(','')\n",
    "ciudad = ciudad.replace('(','')\n",
    "ciudad = ciudad.replace(')','')\n",
    "ciudad = ciudad.replace('/','')\n",
    "ciudad = ciudad.replace('.',' ')\n",
    "ciudad = ciudad.split()\n",
    "\n",
    "stopwords = stopwords + ciudad\n",
    "\n",
    "pattern_1 = r\"[^\\w]\" \n",
    "pattern_2 = r\"[^a-z ]\" \n",
    "\n",
    "def clean_text(text):\n",
    "\n",
    "    text = unidecode(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(pattern_1, \" \", text)\n",
    "    text = re.sub(pattern_2, \" \", text)\n",
    "    text = text.replace('xx', '')\n",
    "    text = ' '.join(word for word in text.split() if word not in stopwords)\n",
    "    \n",
    "    return text\n",
    "\n",
    "#this will be used in app.py and model_train\n",
    "np.save('data/ciudad.npy', ciudad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['descripcion_del_proceso_prueba'] = df['descripcion_del_proceso'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/secop_word_processed.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
